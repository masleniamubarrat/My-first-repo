{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masleniamubarrat/My-first-repo/blob/main/26_11_thesis_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bnlp_toolkit\n"
      ],
      "metadata": {
        "id": "YzEyTxTHoUbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from bnlp import CleanText\n",
        "import string\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "metadata": {
        "id": "SePVAwDpsiep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "17gqiNEJtMOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to your Excel file in your Google Drive\n",
        "file_path = '/content/drive/MyDrive/stopwords_bangla.xlsx'\n",
        "\n",
        "# Read the Excel file\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Extract contents from the 'words' column into a Python list\n",
        "stopwords_list = data['words'].tolist()\n",
        "\n"
      ],
      "metadata": {
        "id": "EikTxBKktMg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pre processing main hate speech"
      ],
      "metadata": {
        "id": "ZLrxpj82ieEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def stemming(text):\n",
        "  words = text.split()\n",
        "  stmr = stemmer.BanglaStemmer()\n",
        "  stm = stmr.stem(words)\n",
        "  sentence = ' '.join(stm)\n",
        "  print(sentence)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def remove_punctuation_and_number(text):\n",
        "    bengali_punctuation = '০১২৩৪৫৬৭৮৯।‘’.-–—॥…,!?;:॰()[]{}\\'\\\"'\n",
        "    words = text.split()\n",
        "\n",
        "    clean_words = []\n",
        "    for word in words:\n",
        "        clean_word = ''.join([char for char in word if char not in bengali_punctuation])\n",
        "        clean_words.append(clean_word)\n",
        "\n",
        "    cleaned_text = ' '.join(clean_words)\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "\n",
        "\n",
        "    stop_words = stopwords_list\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "\n",
        "def preprocess_csv(input_file_path, output_file_path):\n",
        "    # Load the CSV into a DataFrame\n",
        "    data = pd.read_csv(input_file_path)\n",
        "\n",
        "    #Applying stopword removal to the 'text' column\n",
        "    data['stopword_removed_text'] = data['text'].apply(remove_stopwords)\n",
        "    data['punctuation_removed_text'] = data['stopword_removed_text'].apply(remove_punctuation_and_number)\n",
        "    data['punctuation_removed_text'].apply(stemming)\n",
        "\n",
        "\n",
        "     # Create a DataFrame with only the 'preprocessed_text' column\n",
        "    preprocessed_data = data[['punctuation_removed_text']]\n",
        "    return preprocessed_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Define the path to your CSV file\n",
        "    input_file_path = '/content/drive/MyDrive/bengali_hate_v2.0.csv'\n",
        "\n",
        "    # Define the path to save the preprocessed file\n",
        "    output_file_path = '/content/drive/MyDrive/preprocessed_hate_speech.csv'\n",
        "    #stop_words = bengali_stopwords\n",
        "    #print(stop_words)\n",
        "\n",
        "    preprocessed_data =  preprocess_csv(input_file_path, output_file_path)\n",
        "    # Write the preprocessed data to a new CSV file\n",
        "    preprocessed_data.to_csv(output_file_path, index=False)\n",
        "\n",
        "\n",
        "\n",
        "# Call the main function\n",
        "main()"
      ],
      "metadata": {
        "id": "VQGNdtpRs0Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Is4BbOF_HMqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing hate lexicons"
      ],
      "metadata": {
        "id": "YBC1L05TiYWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def stemming(text):\n",
        "    words = text.split()\n",
        "    stmr = stemmer.BanglaStemmer()\n",
        "    stm = stmr.stem(words)\n",
        "    sentence = ' '.join(stm)\n",
        "    print(sentence)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "\n",
        "\n",
        "    stop_words = stopwords_list\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "def preprocess_xlsx(input_file_path, output_file_path):\n",
        "    # Load the Excel file into a DataFrame\n",
        "    data = pd.read_excel(input_file_path)\n",
        "\n",
        "    # Get the column index based on its position (0-indexed)\n",
        "    column_index = 1  # Assuming the 'base_diagram' column is the second column (index 1)\n",
        "\n",
        "    # Applying stopword removal to the 'base_diagram' column using its index\n",
        "    data['processed_text'] = data.iloc[:, column_index].apply(remove_stopwords)\n",
        "\n",
        "    # Selecting 'processed_text' and 'degree_of_toxicity' columns\n",
        "    processed_data = data.iloc[:, [column_index, -1]]  # Assuming 'degree_of_toxicity' is the last column\n",
        "\n",
        "    # Write the processed data to a new CSV file\n",
        "    processed_data.to_csv(output_file_path, index=False)\n",
        "def main():\n",
        "    # Define the path to your Excel file\n",
        "    input_file_path = '/content/drive/MyDrive/hate_lexicons.xlsx'\n",
        "\n",
        "    # Define the path to save the preprocessed CSV file\n",
        "    output_file_path = '/content/drive/MyDrive/preprocessed_hate_lexicons.csv'\n",
        "\n",
        "    preprocess_xlsx(input_file_path, output_file_path)\n",
        "\n",
        "# Call the main function\n",
        "main()\n"
      ],
      "metadata": {
        "id": "OFn7PR5oYqyi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to your Excel file in your Google Drive\n",
        "file_path = '/content/drive/MyDrive/stopwords_bangla.xlsx'\n",
        "\n",
        "# Read the Excel file\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Extract contents from the 'words' column into a Python list\n",
        "stopwords_list = data['words'].tolist()\n",
        "\n"
      ],
      "metadata": {
        "id": "RVhvlxOnoCQ-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GxuX6lwuqIU",
        "outputId": "5fd5f50b-44b3-4f9c-ba6d-71a853e72b17"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bangla-stemmer in /usr/local/lib/python3.10/dist-packages (1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = stopwords_list  # Assuming you have defined stopwords_list somewhere\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "def preprocess_xlsx(input_file_path, output_file_path):\n",
        "    # Load the Excel file into a DataFrame\n",
        "    data = pd.read_excel(input_file_path)\n",
        "\n",
        "    # Get the column index based on its position (0-indexed)\n",
        "    column_index = 1  # Assuming the 'base_diagram' column is the second column (index 1)\n",
        "\n",
        "    # Applying stopword removal to the 'base_diagram' column using its index\n",
        "    data['processed_text'] = data.iloc[:, column_index].apply(remove_stopwords)\n",
        "\n",
        "    # Applying stemming to the 'processed_text' column\n",
        "    data['stemmed_text'] = data['processed_text'].apply(stemming)\n",
        "\n",
        "    # Reordering columns in the desired sequence\n",
        "    processed_data = data.iloc[:, [0, -2, -1]]  # First, processed_text, and last columns\n",
        "\n",
        "    # Write the processed data to a new CSV file\n",
        "    processed_data.to_csv(output_file_path, index=False)\n",
        "\n",
        "def main():\n",
        "    # Define the path to your Excel file\n",
        "    input_file_path = '/content/drive/MyDrive/hate_lexicons.xlsx'\n",
        "\n",
        "    # Define the path to save the preprocessed CSV file\n",
        "    output_file_path = '/content/drive/MyDrive/preprocessed_hate_lexicons.csv'\n",
        "\n",
        "    preprocess_xlsx(input_file_path, output_file_path)\n",
        "\n",
        "# Call the main function\n",
        "main()\n"
      ],
      "metadata": {
        "id": "uVCyKOD5uDiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqlHweb2mrxY",
        "outputId": "4df3e930-da7e-4fae-b9c0-7b6d443acc99"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bangla-stemmer\n",
            "  Downloading bangla_stemmer-1.0-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: bangla-stemmer\n",
            "Successfully installed bangla-stemmer-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wordlist = ['অন্ডকোষ কাটলেই']\n",
        "stmr = stemmer.BanglaStemmer()\n",
        "stm = stmr.stem(wordlist)\n",
        "print(stm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZndDdOsTq58W",
        "outputId": "733d6dcd-2e21-449e-9bcd-57aa89d90937"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "applied first rules..\n",
            "['অন্ডকোষ কাটলে']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bangla-stemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VR4uZ43wq_6",
        "outputId": "3fa48cef-a726-43a2-c888-4066336ccc01"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bangla-stemmer in /usr/local/lib/python3.10/dist-packages (1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from bangla_stemmer.stemmer import stemmer"
      ],
      "metadata": {
        "id": "QhUjSoPrwcSB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nl4W8t3awqfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def stemming(text):\n",
        "  words = text.split()\n",
        "  stmr = stemmer.BanglaStemmer()\n",
        "  stm = stmr.stem(words)\n",
        "  sentence = ' '.join(stm)\n",
        "  return sentence\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = stopwords_list  # Assuming you have defined stopwords_list somewhere\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "def preprocess_xlsx(input_file_path, output_file_path):\n",
        "    # Load the Excel file into a DataFrame\n",
        "    data = pd.read_excel(input_file_path)\n",
        "\n",
        "    # Get the column index based on its position (0-indexed)\n",
        "    column_index = 1  # Assuming the 'base_diagram' column is the second column (index 1)\n",
        "\n",
        "    # Applying stopword removal to the 'base_diagram' column using its index\n",
        "    data['processed_text'] = data.iloc[:, column_index].apply(remove_stopwords)\n",
        "\n",
        "    # Applying stemming to the 'processed_text' column and storing results in a new column 'stemmed_text'\n",
        "    data['stemmed_text'] = data['processed_text'].apply(stemming)\n",
        "\n",
        "    # Reordering columns in the desired sequence\n",
        "    processed_data = data.iloc[:, [0, -1, 8]]  # First, stemmed_text, and last columns\n",
        "\n",
        "    # Write the processed data to a new CSV file\n",
        "    processed_data.to_csv(output_file_path, index=False)\n",
        "\n",
        "def main():\n",
        "    # Define the path to your Excel file\n",
        "    input_file_path = '/content/drive/MyDrive/hate_lexicons.xlsx'\n",
        "\n",
        "    # Define the path to save the preprocessed CSV file\n",
        "    output_file_path = '/content/drive/MyDrive/preprocessed_hate_lexicons.csv'\n",
        "\n",
        "    preprocess_xlsx(input_file_path, output_file_path)\n",
        "\n",
        "# Call the main function\n",
        "main()\n"
      ],
      "metadata": {
        "id": "ok8R6CgLwRIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def map_to_numeric(input_file_path, output_file_path):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    data = pd.read_csv(input_file_path)\n",
        "\n",
        "    # Create a mapping dictionary for the values\n",
        "    mapping = {'Mid': 1, 'High': 2, 'Extreme': 3}\n",
        "\n",
        "    # Map the values in the 'Degree_of_toxicity' column using the dictionary\n",
        "    data['Degree_of_toxicity'] = data['Degree_of_toxicity'].map(mapping)\n",
        "\n",
        "    # Save the updated DataFrame to a new CSV file\n",
        "    data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Define the path to your CSV files\n",
        "input_file_path = '/content/drive/MyDrive/preprocessed_hate_lexicons.csv'\n",
        "output_file_path = '/content/drive/MyDrive/preprocessed_hate_lexicons.csv'\n",
        "\n",
        "# Call the function\n",
        "map_to_numeric(input_file_path, output_file_path)\n"
      ],
      "metadata": {
        "id": "3zoMBgq9ERVc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def extract_unique_words(input_file_path, output_file_path):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    data = pd.read_csv(input_file_path)\n",
        "\n",
        "    # Create an empty DataFrame to store unique words and their details\n",
        "    unique_words = pd.DataFrame(columns=['id', 'first_word', 'degree_of_toxicity'])\n",
        "\n",
        "    # Iterate through each row in the DataFrame\n",
        "    for index, row in data.iterrows():\n",
        "        # Get the stemmed text and split it into words\n",
        "        stemmed_text = row['stemmed_text']\n",
        "\n",
        "        # Check for NaN values in the 'stemmed_text' column\n",
        "        if isinstance(stemmed_text, str):  # Check if the value is a string\n",
        "            words = stemmed_text.split()\n",
        "\n",
        "            # Extract the first word from the row\n",
        "            first_word = words[0]\n",
        "\n",
        "            # Check if the first word is already stored in the unique_words DataFrame\n",
        "            if first_word not in unique_words['first_word'].values:\n",
        "                # If the word is unique, add it to the unique_words DataFrame\n",
        "                unique_words = unique_words.append({\n",
        "                    'id': row['ID'],\n",
        "                    'first_word': first_word,\n",
        "                    'degree_of_toxicity': row['Degree_of_toxicity']\n",
        "                }, ignore_index=True)\n",
        "\n",
        "    # Save the unique words DataFrame to a new CSV file\n",
        "    unique_words.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Define the path to your input and output CSV files\n",
        "input_file_path = '/content/drive/MyDrive/preprocessed_hate_lexicons.csv'\n",
        "output_file_path = '/content/drive/MyDrive/final_hate_lexicon.csv'\n",
        "\n",
        "# Call the function\n",
        "extract_unique_words(input_file_path, output_file_path)\n"
      ],
      "metadata": {
        "id": "XLI-umeqJpsi"
      },
      "execution_count": 41,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}